"""
Redditç›‘æµ‹å·¥å…· - ä¸»å…¥å£
é˜Ÿåˆ—å¤„ç†æ¨¡å¼ï¼šæ”¶é›† â†’ é¢„è¿‡æ»¤ â†’ å…¥é˜Ÿ â†’ å–40æ¡ â†’ AIåˆ†æ â†’ å‘é£ä¹¦

æ¯30åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ï¼Œæ¯æ¬¡åªå¤„ç†40æ¡ï¼ˆ2æ‰¹ï¼‰ï¼Œåˆ†æ•£APIå‹åŠ›
"""

import os
import sys
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.reddit_fetcher import fetch_all_new_posts, load_processed_posts, save_processed_posts
from src.gemini_analyzer import analyze_batch, BATCH_SIZE, REQUEST_DELAY
from src.prefilter import pre_filter
from src.queue_manager import (
    add_to_queue, get_items_to_process, remove_from_queue, 
    get_queue_stats, ITEMS_PER_RUN
)
from src.feishu_notifier import send_batch_to_feishu, send_summary_to_feishu


def count_by_type(items: list) -> dict:
    """ç»Ÿè®¡å„ç±»å‹å†…å®¹æ•°é‡"""
    counts = {'post': 0, 'comment': 0, 'search': 0}
    for item in items:
        t = item.get('type', 'post')
        counts[t] = counts.get(t, 0) + 1
    return counts


def chunk_list(items: list, chunk_size: int) -> list:
    """å°†åˆ—è¡¨åˆ†æˆå›ºå®šå¤§å°çš„å—"""
    return [items[i:i + chunk_size] for i in range(0, len(items), chunk_size)]


def main():
    """ä¸»å‡½æ•° - é˜Ÿåˆ—å¤„ç†æ¨¡å¼"""
    print("=" * 60)
    print(f"Redditç›‘æµ‹å·¥å…·å¯åŠ¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.environ.get('GEMINI_API_KEY'):
        print("[é”™è¯¯] è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    if not os.environ.get('FEISHU_WEBHOOK_URL'):
        print("[é”™è¯¯] è¯·è®¾ç½® FEISHU_WEBHOOK_URL ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # åŠ è½½å·²å¤„ç†è®°å½•
    processed_ids = load_processed_posts()
    
    # ========== é˜¶æ®µ1: æ”¶é›†æ–°å†…å®¹ ==========
    print("\nğŸ“¡ é˜¶æ®µ1: æ”¶é›†Redditæ–°å†…å®¹...")
    new_items = fetch_all_new_posts()
    
    fetch_stats = count_by_type(new_items) if new_items else {}
    
    if new_items:
        # é¢„è¿‡æ»¤
        print("\nğŸ” é¢„è¿‡æ»¤...")
        filtered_items = pre_filter(new_items)
        
        # åŠ å…¥é˜Ÿåˆ—
        if filtered_items:
            added = add_to_queue(filtered_items, processed_ids)
            print(f"  [é˜Ÿåˆ—] æ–°å¢ {added} æ¡å¾…å¤„ç†å†…å®¹")
    else:
        print("  æ²¡æœ‰æ–°å†…å®¹")
    
    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
    queue_stats = get_queue_stats()
    print(f"\nğŸ“‹ é˜Ÿåˆ—çŠ¶æ€: å…± {queue_stats['total']} æ¡å¾…å¤„ç†")
    if queue_stats['total'] > 0:
        print(f"   - é«˜ä¼˜å…ˆçº§(â‰¥3): {queue_stats['by_score']['high']} æ¡")
        print(f"   - ä¸­ä¼˜å…ˆçº§(1-2): {queue_stats['by_score']['medium']} æ¡")
        print(f"   - ä½ä¼˜å…ˆçº§(0): {queue_stats['by_score']['low']} æ¡")
    
    # ========== é˜¶æ®µ2: å¤„ç†é˜Ÿåˆ— ==========
    print(f"\nğŸ¤– é˜¶æ®µ2: å¤„ç†é˜Ÿåˆ—ï¼ˆæœ€å¤š {ITEMS_PER_RUN} æ¡ï¼‰...")
    
    # è·å–å¾…å¤„ç†å†…å®¹
    items_to_process = get_items_to_process(ITEMS_PER_RUN)
    
    if not items_to_process:
        print("  é˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€å¤„ç†")
        print("\nâœ… è¿è¡Œå®Œæˆ!")
        return
    
    print(f"  æœ¬æ¬¡å¤„ç† {len(items_to_process)} æ¡")
    
    # åˆ†æ‰¹å¤„ç†
    batches = chunk_list(items_to_process, BATCH_SIZE)
    total_batches = len(batches)
    
    print(f"  åˆ† {total_batches} æ‰¹ï¼Œæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼Œé—´éš” {REQUEST_DELAY} ç§’")
    print("-" * 50)
    
    # ç»Ÿè®¡
    total_relevant = 0
    total_sent = 0
    processed_item_ids = []
    relevant_stats = {'post': 0, 'comment': 0, 'search': 0}
    
    for batch_idx, batch_items in enumerate(batches):
        batch_num = batch_idx + 1
        
        # åˆ†æå½“å‰æ‰¹æ¬¡
        results = analyze_batch(batch_items, batch_num)
        
        # å¤„ç†åˆ†æç»“æœ
        relevant_in_batch = []
        for result in results:
            if not isinstance(result, dict):
                continue
            
            idx = result.get('index')
            if idx is None or idx >= len(batch_items):
                continue
            
            if result.get('is_relevant', False):
                item = batch_items[idx].copy()
                item['analysis'] = {
                    'is_relevant': True,
                    'reason': result.get('reason', ''),
                    'reply_draft': result.get('reply_draft', '')
                }
                relevant_in_batch.append(item)
                
                # æ›´æ–°ç»Ÿè®¡
                content_type = item.get('type', 'post')
                relevant_stats[content_type] = relevant_stats.get(content_type, 0) + 1
        
        # ç«‹å³å‘é€é£ä¹¦é€šçŸ¥
        if relevant_in_batch:
            sent = send_batch_to_feishu(relevant_in_batch)
            total_sent += sent
            total_relevant += len(relevant_in_batch)
            print(f"  æ‰¹æ¬¡ {batch_num}: å‘ç° {len(relevant_in_batch)} æ¡ç›¸å…³ï¼Œå·²å‘é€é£ä¹¦")
        else:
            print(f"  æ‰¹æ¬¡ {batch_num}: æ— ç›¸å…³å†…å®¹")
        
        # è®°å½•å·²å¤„ç†çš„ID
        for item in batch_items:
            item_id = item.get('id', '')
            if item_id:
                processed_item_ids.append(item_id)
                processed_ids.add(item_id)
        
        # æ¯æ‰¹å¤„ç†åç«‹å³ä¿å­˜ï¼ˆå¢é‡ä¿å­˜ï¼‰
        save_processed_posts(processed_ids)
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ‰¹ï¼Œç­‰å¾…
        if batch_num < total_batches:
            print(f"  ç­‰å¾… {REQUEST_DELAY} ç§’...")
            time.sleep(REQUEST_DELAY)
    
    print("-" * 50)
    
    # ä»é˜Ÿåˆ—ä¸­ç§»é™¤å·²å¤„ç†çš„
    remove_from_queue(processed_item_ids)
    
    # å‘é€æ±‡æ€»é€šçŸ¥
    if total_relevant > 0:
        print("\nğŸ“¤ å‘é€æ±‡æ€»é€šçŸ¥...")
        send_summary_to_feishu({
            'total': len(items_to_process),
            'relevant': total_relevant,
            'sent': total_sent,
            'queue_remaining': queue_stats['total'] - len(processed_item_ids),
            'relevant_posts': relevant_stats.get('post', 0),
            'relevant_comments': relevant_stats.get('comment', 0),
            'relevant_search': relevant_stats.get('search', 0),
        })
    
    # æœ€ç»ˆé˜Ÿåˆ—çŠ¶æ€
    final_stats = get_queue_stats()
    
    # å®Œæˆ
    print("\n" + "=" * 60)
    print("âœ… è¿è¡Œå®Œæˆ!")
    print(f"   æœ¬æ¬¡å¤„ç†: {len(items_to_process)} æ¡")
    print(f"   ç›¸å…³å†…å®¹: {total_relevant} æ¡")
    print(f"   æˆåŠŸæ¨é€: {total_sent} æ¡")
    print(f"   é˜Ÿåˆ—å‰©ä½™: {final_stats['total']} æ¡")
    print("=" * 60)


if __name__ == "__main__":
    main()
